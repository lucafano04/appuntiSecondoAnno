\chapter{\textit{Scheduling} della \texttt{CPU}}

Andremo an analizzare in questo capitolo lo \textit{Scheduling} della \texttt{CPU}, ovvero il modo in cui il sistema operativo decide quale processo eseguire in un dato momento. Lo \textit{Scheduling} è una parte fondamentale del sistema operativo, poiché influisce direttamente sulle prestazioni e sull'efficienza del sistema. Distingueremo inoltre i vari tipi di \textit{Scheduling} (breve, medio e lungo termine) e i vari algoritmi di \textit{Scheduling} (FIFO, SJF, Round Robin, ecc.).

\section{Concetto di \textit{Scheduling}}
    Lo \textit{Scheduling} è il processo di assegnazione di attività nel tempo, l'uso della multiprogrammazione permette di eseguire più processi in parallelo, ma il sistema operativo deve decidere quale processo eseguire in un dato momento, visto che la \texttt{CPU} può eseguire solo un processo alla volta. Bisogna quindi decretare se un programma può essere ammesso nella memoria e quale processo deve essere eseguito in un dato momento. \newline
    Come visto nella sezione \ref{sec:scheduling04}, un processo può trovarsi in uno dei seguenti stati:
    \begin{itemize}
        \item \textbf{New}: il processo è stato creato, ma non è ancora pronto per essere eseguito.
        \item \textbf{Ready}: il processo è pronto per essere eseguito, ma non ha ancora ottenuto l'accesso alla \texttt{CPU}.
        \item \textbf{Running}: il processo sta attualmente eseguendo sulla \texttt{CPU}.
        \item \textbf{Waiting}: il processo è in attesa di un evento esterno (ad esempio, l'input dell'utente o la disponibilità di una risorsa).
        \item \textbf{Terminated}: il processo ha completato la sua esecuzione e sta per essere rimosso dalla memoria.
    \end{itemize}
    Inoltre esistono diverse code di \texttt{Ready} e di \texttt{Waiting}, a seconda del tipo di processo. Ad esempio, i processi in attesa di \texttt{I/O} potrebbero essere in una coda separata rispetto ai processi in attesa di un semaforo. 
    \subsubsection{Implementazione delle Code}
        A livello pratico le code di \texttt{Ready} e di \texttt{Waiting} sono implementate come liste collegate (\textit{linked list}) o come array.\newline Ogni coda ha un \textit{queue header} che contiene informazioni sulla coda stessa, come il puntatore al primo elemento della coda ed il puntatore all'ultimo elemento della coda. Ogni processo ha un \textit{process control block} (\texttt{PCB}) che contiene informazioni sul processo stesso, come il suo stato, il contenuto dei registri quando il processo era in esecuzione, il puntatore al prossimo processo da eseguire, ecc\dots \newline
        Una coda può essere o la coda di \textit{ready}, dove i processi pronti per essere eseguiti sono in attesa di essere assegnati alla \texttt{CPU}, oppure una delle code di \textit{waiting}, dove i processi sono in attesa di un evento esterno ed ogni coda rappresenta un evento diverso.

\section{Tipi di \textit{Scheduling}}
    Per la gestione dei processi si possono distinguere tre tipi di \textit{scheduling}, di cui due principali ed uno secondario:
    \begin{itemize}
        \item \textit{\textbf{Long-term scheduling}} - Pianificazione a lungo termine
        \item \textit{\textbf{Short-term scheduling}} - Pianificazione a breve termine
        \item \textit{\textbf{Medium-term scheduling}} - Pianificazione a medio termine
    \end{itemize}
    \subsubsection{Pianificazione a lungo termine - (\textit{job-scheduler})}
        La pianificazione a lungo termine è il processo di selezione dei processi da ammettere nella memoria principale. Questo tipo di pianificazione è responsabile della creazione di nuovi processi e della loro ammissione nella memoria, e dunque nella coda \textit{ready}, per essere eseguiti. La pianificazione a lungo termine determina il grado di multiprogrammazione del sistema, ovvero il numero di processi che possono essere eseguiti contemporaneamente. Se il grado di multiprogrammazione è troppo alto, il sistema potrebbe diventare instabile e i processi potrebbero non ricevere le risorse necessarie per essere eseguiti. Se il grado di multiprogrammazione è troppo basso, la \texttt{CPU} potrebbe rimanere inattiva per lunghi periodi di tempo, riducendo l'efficienza del sistema. Inoltre il \textit{long-term scheduler} è responsabile della determinazione del tipo di processo da eseguire, ovvero se questo è \texttt{CPU} \textit{bound} oppure \texttt{I/O} \textit{bound}.
        \paragraph{Frequenza} La pianificazione a lungo termine viene eseguita con frequenza dell'ordine del secondo o di pochi secondi, poiché la creazione di nuovi processi e la loro ammissione nella memoria sono operazioni relativamente costose in termini di tempo e risorse.\newline
        Questo sistema è opzionale e può essere assente.
    \subsubsection{Pianificazione a breve termine - \textit{\texttt{CPU}-scheduler}}
        La pianificazione a breve termine è il processo di selezione del processo da eseguire sulla \texttt{CPU} in un dato momento. Questo tipo di pianificazione è responsabile della gestione dei processi in esecuzione e della loro assegnazione alla \texttt{CPU}. La pianificazione a breve termine determina quale processo deve essere eseguito in un dato momento, in base a diversi criteri, come la priorità del processo, il tempo di attesa e il tempo di completamento. La pianificazione a breve termine è responsabile della gestione della \texttt{CPU} e della sua assegnazione ai processi in esecuzione.
        \paragraph{Frequenza} La pianificazione a breve termine viene eseguita con frequenza dell'ordine dei millisecondi, dunque deve essere una operazione molto veloce, infatti se il tempo di processo è $100ms$ ed il tempo di \textit{scheduling} è $10ms$, il tempo di \textit{scheduling} incide per il 9\% sul tempo totale di esecuzione del processo. Se il tempo di \textit{scheduling} è troppo alto, il sistema potrebbe diventare instabile e i processi potrebbero non ricevere le risorse necessarie per essere eseguiti. Se il tempo di \textit{scheduling} è troppo basso, la \texttt{CPU} potrebbe rimanere inattiva per lunghi periodi di tempo, riducendo l'efficienza del sistema.\newline
        Questo sistema è sempre presente e non può essere assente, poiché è necessario per la gestione della \texttt{CPU} e dei processi in esecuzione.
    \subsubsection{Pianificazione a medio termine - \textit{medium-term scheduler}}
        La pianificazione a medio termine è un processo intermedio tra la pianificazione a lungo termine e la pianificazione a breve termine. Questo è presente se e solo se il sistema operativo supporta la \textit{swapping}, ovvero il trasferimento di processi dalla memoria principale alla memoria secondaria (disco) e viceversa. La porzione di disco usata per questo scopo è chiamata \textit{swap space} ed sostanzialmente è della \texttt{RAM} virtuale che viene usata per memorizzare i processi che non sono attualmente in esecuzione. La pianificazione a medio termine è responsabile della gestione della memoria e della sua assegnazione ai processi in esecuzione. Questo tipo di pianificazione è responsabile dell'immagazzinamento dei processi nella memoria secondaria e del loro trasferimento nella memoria principale quando necessario. Questo processo viene eseguito con tutti i processi che escono dalla \texttt{CPU} per rientrare nella \textit{ready queue} ma non può avvenire se il processo esce dalla \texttt{CPU} per inserirsi nella \textit{waiting queue}.


\section{Scheduling della \texttt{CPU}}
    \paragraph{\textit{Scheduler}}Lo \textit{scheduler} della \texttt{CPU} è, a livello logico, il modulo del \texttt{SO} che decide quale processo eseguire in un dato momento, vista la frequenza di chiamate a funzioni di \textit{Scheduling} e la velocità con cui i processi passano da uno stato all'altro, lo \textit{scheduler} deve essere molto veloce.
    \paragraph{\textit{Dispatcher}} Il \textit{dispatcher} è il modulo del \texttt{SO} che effettivamente esegue il passaggio di controllo tra i processi, ovvero il passaggio da un processo all'altro. Il \textit{dispatcher} è responsabile di:
    \begin{itemize}
        \item \textit{Switch} del contesto: salva il contesto del processo corrente e carica il contesto del processo successivo.
        \item Passaggio alla modalità utente: il \texttt{SO} deve passare dalla modalità kernel alla modalità utente per eseguire il processo.
        \item Salto alla opportuna locazione nel codice: il \texttt{dispatcher} deve saltare alla locazione corretta nel codice del processo.
    \end{itemize}
    La latenza di un \textit{dispatcher} consiste nel tempo necessario per eseguire queste operazioni, ovvero fermare il processo corrente e passare al successivo. La latenza del \textit{dispatcher} è molto importante, poiché influisce sulle prestazioni del sistema. Un \textit{dispatcher} veloce può migliorare le prestazioni del sistema, mentre un \textit{dispatcher} lento può causare un degrado delle prestazioni.
    \subsubsection{Modello astratto del sistama} 
        Quando parliamo di un processo a livello astratto consideriamo che questo possa essere o in \texttt{CPU} \textit{burst} oppure in \texttt{I/O} \textit{burst}
        \paragraph{Distribuzione dei \texttt{CPU} \textit{burst}} Solitamente i processi hanno una distribuzione dei \texttt{CPU} \textit{burst} che segue una distribuzione esponenziale, ovvero la maggior parte dei processi ha un \texttt{CPU} \textit{burst} breve, mentre pochi processi hanno un \texttt{CPU} \textit{burst} lungo. Questo è dovuto al fatto che i processi brevi sono più comuni rispetto ai processi lunghi. Per questo motivo è stato implementato il processo di prelazione (\textit{preemption})
        \paragraph{Prelazione} Come detto in precedenza, i processi brevi sono più comuni rispetto ai processi lunghi, per questo motivo è stato implementato il processo di prelazione (\textit{preemption}), ovvero la possibilità di interrompere un processo in esecuzione per dare la precedenza ad un altro processo. Esistono dunque in circolazione due tipi di \textit{scheduler}:
        \begin{itemize}
            \item \textbf{Non preemptive}: il processo in esecuzione non può essere interrotto, ma deve terminare la sua esecuzione prima di passare al successivo.
            \item \textbf{Preemptive}: il processo in esecuzione può essere interrotto in qualsiasi momento per dare la precedenza ad un altro processo.
        \end{itemize}
        La prelazione è utile per garantire che i processi brevi vengano eseguiti il prima possibile, evitando che i processi lunghi occupino la \texttt{CPU} per troppo tempo. Tuttavia, la prelazione può anche causare un aumento della latenza del \textit{dispatcher}, poiché il \textit{dispatcher} deve eseguire il passaggio di controllo tra i processi più frequentemente.
    \subsubsection{Metriche di \textit{scheduling}}
        Esistono diverse metriche sulle quali scegliere un algoritmo di \textit{scheduling} piuttosto che un altro, le più comuni sono:
        \begin{itemize}
            \item \textbf{Utilizzo della \texttt{CPU}} (\textit{CPU Utilization}): percentuale di tempo in cui la \texttt{CPU} è occupata ad eseguire processi. Un utilizzo della \texttt{CPU} del 100\% è l'ideale, ma è difficile da raggiungere.
            \item \textbf{\textit{Throughput}}: numero di processi completati in un dato intervallo di tempo. Un \textit{throughput} elevato è desiderabile, poiché indica che il sistema sta eseguendo molti processi in un breve periodo di tempo.
            \item \textbf{Tempo di attesa} (\textit{Waiting Time}): tempo medio che un processo trascorre in attesa di essere eseguito. Un tempo di attesa basso è desiderabile, poiché indica che i processi vengono eseguiti rapidamente.
            \item \textbf{Tempo di completamento} (\textit{Turnaround Time}): tempo medio che un processo trascorre nel sistema, dalla sua creazione alla sua terminazione. Un tempo di completamento basso è desiderabile, poiché indica che i processi vengono eseguiti rapidamente.
            \item \textbf{Tempo di risposta} (\textit{Response Time}): tempo medio che intercorre tra l'invio di una richiesta e la ricezione della risposta. Un tempo di risposta basso è desiderabile, poiché indica che il sistema risponde rapidamente alle richieste degli utenti.
        \end{itemize}
        Il compito di un algoritmo di \textit{scheduling} è quello di massimizzare l'utilizzo della \texttt{CPU} e il \textit{throughput}, minimizzando il tempo di attesa, il tempo di completamento e il tempo di risposta. Tuttavia, non è sempre possibile ottimizzare tutte queste metriche contemporaneamente, poiché spesso ci sono compromessi tra di esse. Ad esempio, un algoritmo che massimizza l'utilizzo della \texttt{CPU} potrebbe aumentare il tempo di attesa dei processi, mentre un algoritmo che minimizza il tempo di attesa potrebbe ridurre l'utilizzo della \texttt{CPU}.
    \subsection{Algoritmi di \textit{scheduling}}
        Andiamo ora ad analizzare i vari algoritmi di \textit{scheduling} della \texttt{CPU}, partendo da quelli più semplici e passando a quelli più complessi.
        \subsubsection{\textit{First-Come, First-Served} (\texttt{FCFS})}
            L'algoritmo \texttt{FCFS} è il più semplice degli algoritmi di \textit{scheduling}, i processi vengono eseguiti nell'ordine in cui arrivano nella coda di \texttt{Ready}. Questo algoritmo è semplice da implementare e non richiede alcun calcolo complesso. Tuttavia, ha alcuni svantaggi:
            \begin{itemize}
                \item Non tiene conto della lunghezza dei processi, quindi i processi lunghi possono bloccare l'esecuzione dei processi brevi.
                \item Può causare un aumento del tempo di attesa e del tempo di completamento per i processi brevi.
            \end{itemize}
            L'algoritmo \texttt{FCFS} è un algoritmo non preemptive, poiché un processo in esecuzione non può essere interrotto fino al suo completamento. Questo può portare a una bassa efficienza del sistema, poiché i processi brevi possono rimanere in attesa per lungo tempo.
            \paragraph{Esempio} consideriamo questi processi:

            \begin{table}[H]
                \centering
                \begin{tabular}{|c|c|c|}
                    \hline
                    \textbf{Processo} & \textbf{Tempo di arrivo} & \textbf{\textit{\texttt{CPU} burst}} \\ \hline
                    P1 & 0 & 24 \\ \hline
                    P2 & 2 & 3 \\ \hline
                    P3 & 4 & 3 \\ \hline
                \end{tabular}
            \end{table}
            Allora i tempi di attesa, completamento e di risposta sono:
            \begin{table}[H]
                \centering
                \begin{tabular}{|c|c|c|c|c|}
                    \hline
                    \textbf{Processo} & \textbf{$T_r$} & \textbf{$T_w$} & \textbf{$T_t$} \\ \hline
                    P1 & 0 & 0 & 24 \\ \hline
                    P2 & 24 & 22 & 25 \\ \hline
                    P3 & 27 & 23 & 30 \\ \hline
                \end{tabular}
            \end{table}
            Dunque il tempo medio di attesa è:
            $$
                T_{w,med} = \frac{T_{w,P1} + T_{w,P2} + T_{w,P3}}{3} = \frac{0 + 22 + 23}{3} = 15$$
            Il tempo medio di completamento è:
            $$
                T_{t,med} = \frac{T_{t,P1} + T_{t,P2} + T_{t,P3}}{3} = \frac{24 + 25 + 30}{3} = 26$$
            Se però cambiano i tempi di arrivo dei processi, ad esempio:
            \begin{table}[H]
                \centering
                \begin{tabular}{|c|c|c|}
                    \hline
                    \textbf{Processo} & \textbf{Tempo di arrivo} & \textbf{\textit{\texttt{CPU} burst}} \\ \hline
                    P1 & 4 & 24 \\ \hline
                    P2 & 0 & 3 \\ \hline
                    P3 & 2 & 3 \\ \hline
                \end{tabular}
            \end{table}
            Allora i tempi di attesa, completamento e di risposta sono:
            \begin{table}[H]
                \centering
                \begin{tabular}{|c|c|c|c|}
                    \hline
                    \textbf{Processo} & \textbf{$T_r$} & \textbf{$T_w$} & \textbf{$T_t$} \\ \hline
                    P1 & 2 & 2 & 26 \\ \hline
                    P2 & 0 & 0 & 3 \\ \hline
                    P3 & 1 & 1 & 4 \\ \hline
                \end{tabular}
            \end{table}
            Dunque il tempo medio di attesa è:
            $$
                T_{w,med} = \frac{T_{w,P1} + T_{w,P2} + T_{w,P3}}{3} = \frac{2 + 0 + 1}{3} = 1
            $$
            Il che è molto più veloce rispetto al caso precedente, nonostante il processo P1 sia più lungo. Questo è dovuto al fatto che i processi brevi sono stati eseguiti prima di P1, riducendo il tempo di attesa per P1.
        \subsubsection{\textit{Shortest Job First} (\texttt{SJF})}
            L'algoritmo \texttt{SJF} è un algoritmo di \textit{scheduling} che assegna la \texttt{CPU} al processo con il \textit{\texttt{CPU} burst} più breve. Questo algoritmo è in grado di ridurre il tempo di attesa e il tempo di completamento dei processi, poiché i processi brevi vengono eseguiti per primi. Questo algoritmo può essere implementato sia in modo \textit{preemptive} che in modo non \textit{preemptive}. Se è implementato in modo \textit{preemptive}, il processo in esecuzione può essere interrotto se arriva un processo con un \textit{\texttt{CPU} burst} più breve rispetto al \texttt{CPU} \textit{burst} \underline{rimanente} del processo in esecuzione (\textit{Shortest-Remaining-Time-First} - \texttt{SRTF}). Se è implementato in modo non \textit{preemptive}, il processo in  esecuzione non può essere interrotto fino al suo completamento.
            \paragraph{Esempio} consideriamo questi processi:
            \begin{table}[H]
                \centering
                \begin{tabular}{|c|c|c|}
                    \hline
                    \textbf{Processo} & \textbf{Tempo di arrivo} & \textbf{\textit{\texttt{CPU} burst}} \\ \hline
                    P1 & 0 & 7 \\ \hline
                    P2 & 2 & 4 \\ \hline
                    P3 & 4 & 1 \\ \hline
                    P4 & 5 & 4 \\ \hline
                    \end{tabular}
            \end{table}
            Allora i processi verranno eseguiti in questo ordine:
            \begin{itemize}
                \item P1 (0-7)
                \item P3 (7-8)
                \item P2 (8-12)
                \item P4 (12-16)
            \end{itemize}
            Creando quindi i seguenti tempi di attesa, completamento e di risposta:
            \begin{table}[H]
                \centering
                \begin{tabular}{|c|c|c|c|}
                    \hline
                    \textbf{Processo} & \textbf{$T_r$} & \textbf{$T_w$} & \textbf{$T_t$} \\ \hline
                    P1 & 0 & 0 & 7 \\ \hline
                    P2 & 6 & 6 & 10 \\ \hline
                    P3 & 3 & 3 & 4 \\ \hline
                    P4 & 7 & 7 & 11 \\ \hline
                \end{tabular}
            \end{table}
            Dunque il tempo medio di attesa è:
            $$
                T_{w,med} = \frac{T_{w,P1} + T_{w,P2} + T_{w,P3} + T_{w,P4}}{4} = \frac{0 + 6 + 3 + 7}{4} = 4
            $$
            Se gli stessi processi arrivano nello stesso ordine ma in un sistema \textit{preemptive}, i processi verranno eseguiti in questo ordine:
            \begin{itemize}
                \item P1 (0-2) - 5 rimasti
                \item P2 (2-4) - 2 rimasti
                \item P3 (4-5)
                \item P2 (5-7)
                \item P4 (7-11)
                \item P1 (11-16)
            \end{itemize}
            Creando quindi i seguenti tempi di attesa, completamento e di risposta:
            \begin{table}[H]
                \centering
                \begin{tabular}{|c|c|c|c|}
                    \hline
                    \textbf{Processo} & \textbf{$T_r$} & \textbf{$T_w$} & \textbf{$T_t$} \\ \hline
                    P1 & 0 & 0 & 16 \\ \hline
                    P2 & 5 & 5 & 10 \\ \hline
                    P3 & 4 & 4 & 5 \\ \hline
                    P4 & 7 & 7 & 11 \\ \hline
                \end{tabular}
            \end{table}
            Dunque il tempo medio di attesa è:
            $$
                T_{w,med} = \frac{T_{w,P1} + T_{w,P2} + T_{w,P3} + T_{w,P4}}{4} = \frac{0 + 5 + 4 + 7}{4} = 4
            $$
            Il che è lo stesso del caso non \textit{preemptive}, ma in questo caso il tempo di attesa è più basso per i processi brevi, mentre il tempo di attesa per i processi lunghi è più alto.\newline
            Il principale problema di questo algoritmo è quello che è impossibile determinare con precisione il \textit{\texttt{CPU} burst} di un processo, poiché questo dipende da molti fattori esterni. Viene dunque usata una media esponenziale (\textit{exponential average}) per stimare il \textit{\texttt{CPU} burst} di un processo. La media esponenziale è una media che dà più peso ai valori recenti rispetto ai valori più vecchi. La formula per calcolare la media esponenziale è:
            $$T_{n+1} = \alpha T_n + (1 - \alpha) T_{n-1}$$
            dove:
            \begin{itemize}
                \item $T_{n+1}$ è il nuovo valore della media esponenziale
                \item $T_n$ è il valore corrente del \textit{\texttt{CPU} burst}
                \item $T_{n-1}$ è il valore precedente del \textit{\texttt{CPU} burst}
                \item $\alpha$ è un valore compreso tra 0 e 1 che determina il peso dei valori recenti rispetto ai valori più vecchi. Un valore di $\alpha$ vicino a 1 dà più peso ai valori recenti, mentre un valore di $\alpha$ vicino a 0 dà più peso ai valori più vecchi.
            \end{itemize}
        \subsubsection{\textit{Scheduling} a priorità}
            Nello \textit{scheduling} a priorità ad ogni processo viene associata una priorità, i processi con priorità più alta vengono eseguiti per primi. Questo algoritmo può essere implementato sia in modo \textit{preemptive} che in modo non \textit{preemptive}. Un esempio di \textit{scheduling} con priorità è il comando \texttt{nice} di \texttt{Linux}, che permette di modificare la priorità di un processo.
            \paragraph{Politiche di assegnamento priorità} L'assegnamento di un livello di priorità rispetto ad un altro può essere influenzato da fattori interni o esterni al \texttt{SO}. Come fattori interni troviamo ad esempio: limiti di tempo, requisiti di memoria, numero di file richiesti, \dots\newline Possono anche essere influenzati da fattori esterni, come ad esempio l'importanza (umana) del processo, se per quel determinato processo si ha un guadagno economico o anche motivi politici, \dots
            \paragraph{Problemi} Il principale problema dei \texttt{SO} con \textit{scheduling} a priorità è la \textit{starvation}, ovvero certi processi a bassa priorità potrebbero non essere mai eseguiti in quanto ci sono sempre processi con priorità più alta in attesa di essere eseguiti. Questo problema può essere risolto utilizzando una tecnica chiamata \textit{aging}, che consiste nell'aumentare gradualmente la priorità dei processi a bassa priorità man mano che trascorrono del tempo in attesa. In questo modo, anche i processi a bassa priorità avranno la possibilità di essere eseguiti, evitando la \textit{starvation}.
            \paragraph{\textit{Higher Response Ratio Next} - \texttt{HRRN}} L'\texttt{HRRN} è un algoritmo di \textit{scheduling} a priorità, \underline{sempre} \underline{non-\textit{preentive}}. In questo algoritmo la priorità di un processo viene calcolata in base al suo tempo di attesa e al suo \textit{\texttt{CPU} burst}. La formula per calcolare la priorità di un processo è: \[
                R = \frac{T_w + T_{CPU}}{T_{CPU}}
            \]
            il maggiore valore di $R$ avrà la priorità più alta. Questo algoritmo è in grado di ridurre il tempo di attesa e il tempo di completamento dei processi, poiché i processi brevi vengono eseguiti per primi. Inoltre se un processo è in attesa per lungo tempo, la sua priorità aumenta, evitando la \textit{starvation} e la priorità è dunque dinamica. Se alla fine di un processo è arrivato un altro processo allora la priorità dei processi in attesa viene ricalcolata, andando a modificare l'ordine di esecuzione dei processi se necessario, oppure può anche essere ricalcolata alla fine di ogni \texttt{CPU} \textit{burst} indipendentemente dal fatto che sia arrivato un nuovo processo o meno (dipende dalle implementazioni).
            \paragraph{Esempio} consideriamo questi processi:
            \begin{table}[H]
                \centering
                \begin{tabular}{|c|c|c|}
                    \hline
                    \textbf{Processo} & \textbf{Tempo di arrivo} & \textbf{\textit{\texttt{CPU} burst}} \\ \hline
                    P1 & 1 & 10 \\ \hline
                    P2 & 0 & 2 \\ \hline
                    P3 & 2 & 2 \\ \hline
                    P4 & 2 & 1 \\ \hline
                    P5 & 1 & 5 \\ \hline
                \end{tabular}
            \end{table}
            Allora il calcolo della priorità dei processi è il seguente:
            \begin{table}[H]
                \centering
                \begin{tabular}{|c|c|c|c|c|}
                    \hline 
                    \textbf{Processo} & $t=0$ & $t=2$ & $t=7$ & $t=8$ \\ \hline
                    P1 & - & 1+1/10 & 1+6/10 & 1+7/10 \\ \hline
                    P2 & 1 & - & - & - \\ \hline
                    P3 & - & 1 + 0/2 & 1 + 5/2 & 1 + 6/2 \\ \hline
                    P4 & - & 1 + 0/1 & 1 + 5/1 & - \\ \hline
                    P5 & - & 1 + 1/5 & - & - \\ \hline
                \end{tabular}
            \end{table}\footnote{Nota come nel seguente esempio il calcolo della priorità è stato fatto anche per i tempi $t=7$ e $t=8$, anche se non sono arrivati nuovi processi.}
            \subsubsection{\textit{Round Robin} (\texttt{RR})}
                L'algoritmo \texttt{RR} è un algoritmo \textit{scheduling} \textit{preemptive} che assegna un piccolo tempo di \texttt{CPU} chiamato \textit{quantum} ($10-100\ \operatorname{ms}$) ad ogni processo in coda. Quando il tempo di \texttt{CPU} di un processo scade, il processo viene interrotto e messo in coda, e il \textit{dispatcher} passa al processo successivo. Questo algoritmo è in grado di garantire una buona risposta per i processi interattivi, poiché i processi brevi vengono eseguiti rapidamente. Tuttavia bisogna fare una scelta sul valore del \textit{quantum}, se questo è molto grande allora è esattamente come l'algoritmo \textit{First Come First Serve}, se invece è molto piccolo allora il tempo di \textit{scheduling} aumenta, poiché ad ogni passaggio di processo il \textit{dispatcher} deve eseguire il \textit{context switch} e passare alla modalità utente, il valore ottimale per il tempo $q$ deve essere scelto in modo che il tempo $q$ sia minore dell'$80\%$ dei \textit{\texttt{CPU} burst}
                \paragraph{Quanto-\textit{context-switch}} Esiste una relazione direttamente inversa tra il numero di \textit{context switch} e il tempo di \texttt{CPU} \textit{burst}, ovvero più è lungo il \textit{\texttt{CPU} burst} e meno \textit{context switch} ci sono. Questo è dovuto al fatto che ogni volta che si esegue un \textit{context switch} il sistema deve salvare lo stato del processo corrente e caricare lo stato del processo successivo, il che richiede tempo e risorse. Se i processi hanno un \textit{\texttt{CPU} burst} lungo, ci saranno meno \textit{context switch} e quindi meno tempo sprecato.
                \paragraph{Quanto-Tempo di attesa} A differenza del numero di \textit{context-switch}, il tempo di attesa \underline{non è} legato direttamente al tempo di \textit{quantum}, ma è legato al numero di processi in coda e ai tempi di esecuzione dei processi stessi. Se ci sono molti processi in coda, il tempo di attesa per ciascun processo aumenta, poiché ogni processo deve attendere il completamento del \textit{quantum} degli altri processi prima di essere eseguito nuovamente. Tuttavia, un \textit{quantum} troppo piccolo può aumentare il tempo di attesa complessivo a causa dell'overhead introdotto dai frequenti \textit{context-switch}.
            \subsubsection{Code multi-livello}
                Le code multi-livello sono una suddivisione della \textit{ready queue} in più code, ognuna con un ruolo specifico ed un algoritmo di \textit{scheduling} specifico. Ad esempio si potrebbe avere una coda per i processi interattivi, una coda per i processi batch e una coda per i processi in background, queste potrebbero essere gestite con \texttt{RR}, \texttt{FCFS} e \texttt{SJF} rispettivamente. In questo modo si può garantire una buona risposta per i processi interattivi e una buona efficienza per i processi batch. Questo però al costo di dover gestire lo \textit{scheduling} tra le varie code. Quest'ultimo solitamente è gestito o come \textit{time slice}, ovvero ogni coda ha una percentuale di tempo di \texttt{CPU} da utilizzare, oppure tramite priorità fissa, ovvero ogni coda ha una priorità fissa e la coda con priorità più alta viene eseguita, e liberata, prima delle altre. 
                \paragraph{Code multi-livello a \textit{feedback}} Mentre in una coda multi-livello tradizionale quando un processo viene assegnato ad una coda specifica non può cambiare coda, in una coda multi-livello a \textit{feedback} i processi possono cambiare coda sulla base delle proprie caratteristiche di esecuzione, spesso ciò viene fatto per implementare l'\textit{aging} e per evitare la \textit{starvation}. I parametri sui quali si può agire per la configurazione dello \textit{scheduler} sono: il numero delle code, l'algoritmo usato, i criteri di promozione e/o retrocessione dei processi ed i criteri per definire la coda iniziale di un processo.
            \subsubsection{\textit{Scheduling fair share}}
                Dato che un programmatore sapendo che in alcuni casi se pianifica molti \textit{threads} allora il suo processo potrebbe ottenere più tempo di \texttt{CPU} rispetto ad un altro processo, è stato implementato lo \textit{scheduling fair share}, che lavora per applicazione e non per processo. Ad ogni applicazione viene assegnato una percentuale del tempo di \texttt{CPU} e i processi di quell'applicazione possono usare solo quella percentuale del tempo di \texttt{CPU}. Questo algoritmo è in grado di garantire che ogni applicazione riceva una quantità equa di tempo di \texttt{CPU}, evitando che un'applicazione monopolizzi le risorse del sistema. Tuttavia, questo algoritmo può causare un aumento del tempo di attesa per i processi ed un aumento del tempo di completamento, poiché i processi devono attendere che il loro turno arrivi. Inoltre, questo algoritmo può essere più complesso da implementare rispetto ad altri algoritmi di \textit{scheduling}, poiché richiede la gestione delle percentuali di tempo di \texttt{CPU} per ogni applicazione e la loro assegnazione ai processi.
            \subsubsection{Contesto reale}
                Solitamente nei sistemi operativi moderni viene usato un mix di algoritmi di \textit{scheduling}, ad esempio il \texttt{Linux} usa un algoritmo \textit{completely fair scheduler} (\texttt{CFS}) che è una combinazione di \texttt{RR} e \texttt{SJF}. Questo algoritmo è in grado di garantire una buona risposta per i processi interattivi e una buona efficienza per i processi batch. Inoltre, questo algoritmo è in grado di adattarsi alle diverse condizioni del sistema, modificando dinamicamente le priorità dei processi in base al loro comportamento.
        \subsection{Valutazione degli algoritmi}
            Esistono diversi metodi per valutare le prestazioni degli algoritmi di \textit{scheduling}, noi affrontiamo il modello deterministico ed il modello a reti di code.\
            \subsubsection{Modello deterministico (Analitico)}
                Il modello deterministico è un metodo di valutazione degli algoritmi di \textit{scheduling} che si basa su un insieme di processi con tempi di arrivo e tempi di esecuzione noti.\footnote{Come è stato fatto in precedenza con i vari esempi.} Questo metodo consente di calcolare le prestazioni degli algoritmi di \textit{scheduling} in modo preciso e dettagliato, poiché i tempi di arrivo e i tempi di esecuzione sono noti in anticipo. Tuttavia, questo metodo non tiene conto delle variazioni nei tempi di arrivo e nei tempi di esecuzione dei processi, il che può portare a risultati poco realistici. Inoltre, questo metodo richiede una conoscenza dettagliata dei processi e delle loro caratteristiche, il che può essere difficile da ottenere in un sistema reale. Per questo motivo, il modello deterministico è più adatto per la valutazione di algoritmi di \textit{scheduling} in ambienti controllati e non in ambienti reali.
            \subsubsection{Modello a reti di code} 
                Il modello a reti di code è un metodo di valutazione degli algoritmi di \textit{scheduling} che si basa su un preciso numero di processi sempre uguali ed con tempi di \texttt{CPU} \textit{burst}, \texttt{I/O} \textit{burst} ed arrivo basati su una distribuzione di Poisson della quale si varia il parametro $\lambda$. Da questo modello è possibile calcolare il tempo medio di attesa, il tempo medio di completamento e il tempo medio di risposta per ogni algoritmo di \textit{scheduling}. Questo metodo consente di valutare le prestazioni degli algoritmi di \textit{scheduling} in modo più realistico rispetto al modello deterministico, poiché tiene conto delle variazioni nei tempi di arrivo e nei tempi di esecuzione dei processi. Tuttavia, questo metodo richiede una conoscenza dettagliata delle distribuzioni dei processi e delle loro caratteristiche, il che può essere difficile da ottenere in un sistema reale. Per questo motivo, il modello a reti di code è più adatto per la valutazione di algoritmi di \textit{scheduling} in ambienti reali e non in ambienti controllati.
            \subsubsection{Simulazione}
                La simulazione è un metodo di valutazione degli algoritmi di \textit{scheduling} che si basa sull'esecuzione di un insieme di processi in un ambiente simulato. Questo metodo consente di valutare le prestazioni degli algoritmi di \textit{scheduling} in modo realistico, poiché tiene conto delle variazioni nei tempi di arrivo e nei tempi di esecuzione dei processi. Inoltre, questo metodo consente di testare gli algoritmi di \textit{scheduling} in condizioni controllate ma pseudo-reali, il che può essere utile per la valutazione di algoritmi di \textit{scheduling} in ambienti reali. Tuttavia, questo metodo richiede che il modello del sistema sia già disponibile, inoltre il suo uso seppur garantendo una buona valutazione delle prestazioni degli algoritmi di \textit{scheduling} può essere costoso in termini di tempo e risorse.
            \subsubsection{Implementazione}
                L'implementazione di un algoritmo di \textit{scheduling} è l'unico metodo certo per valutare le prestazioni di un algoritmo di \textit{scheduling} in un sistema reale. Questo metodo consente di testare gli algoritmi di \textit{scheduling} in condizioni reali e concrete e di valutare le loro prestazioni in un ambiente reale. Tuttavia, questo metodo richiede che questo algoritmo sia già codificato, inserito nel \texttt{SO} e solo dopo è possibile verificarne l'effettiva efficienza. Inoltre, questo metodo può essere costoso in termini di tempo e risorse, poiché richiede la modifica del \texttt{SO} e la sua re-installazione. Tuttavia, l'implementazione di un algoritmo di \textit{scheduling} è il metodo più preciso e affidabile per valutare le prestazioni degli algoritmi di \textit{scheduling} in un sistema reale.